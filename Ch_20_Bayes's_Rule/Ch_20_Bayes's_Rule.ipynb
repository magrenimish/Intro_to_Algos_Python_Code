{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Naive Bayes Classifier"
      ],
      "metadata": {
        "id": "UWDaUNfqDwZP"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_vnZQjlPBO9m"
      },
      "outputs": [],
      "source": [
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "from sklearn.metrics import accuracy_score, classification_report\n",
        "\n",
        "# Load the dataset\n",
        "data = load_iris()\n",
        "X, y = data.data, data.target\n",
        "\n",
        "# Split the dataset into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Create a Gaussian Naive Bayes classifier\n",
        "gnb = GaussianNB()\n",
        "\n",
        "# Train the classifier\n",
        "gnb.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions on the test set\n",
        "y_pred = gnb.predict(X_test)\n",
        "\n",
        "# Evaluate the classifier\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "report = classification_report(y_test, y_pred)\n",
        "\n",
        "print(\"Accuracy:\", accuracy)\n",
        "print(\"Classification Report:\")\n",
        "print(report)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Metropolis-Hastings Algorithm"
      ],
      "metadata": {
        "id": "BKabm3UdD4dH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "def target_distribution(x):\n",
        "    # Example target distribution: Standard normal distribution\n",
        "    return np.exp(-0.5 * x**2) / np.sqrt(2 * np.pi)\n",
        "\n",
        "def proposal_distribution(x, sigma):\n",
        "    # Example proposal distribution: Normal distribution centered at x with standard deviation sigma\n",
        "    return np.random.normal(x, sigma)\n",
        "\n",
        "def metropolis_hastings(target, proposal, initial_value, num_samples, sigma):\n",
        "    samples = [initial_value]\n",
        "    current_value = initial_value\n",
        "\n",
        "    for _ in range(num_samples):\n",
        "        proposed_value = proposal(current_value, sigma)\n",
        "        acceptance_ratio = target(proposed_value) / target(current_value)\n",
        "        if np.random.rand() < acceptance_ratio:\n",
        "            current_value = proposed_value\n",
        "        samples.append(current_value)\n",
        "\n",
        "    return np.array(samples)\n",
        "\n",
        "# Parameters\n",
        "initial_value = 0\n",
        "num_samples = 10000\n",
        "sigma = 1.0\n",
        "\n",
        "# Run Metropolis-Hastings algorithm\n",
        "samples = metropolis_hastings(target_distribution, proposal_distribution, initial_value, num_samples, sigma)\n",
        "\n",
        "# Plot results\n",
        "plt.figure(figsize=(12, 6))\n",
        "plt.hist(samples, bins=50, density=True, alpha=0.6, color='g', label='Samples')\n",
        "x = np.linspace(-4, 4, 1000)\n",
        "plt.plot(x, target_distribution(x), 'r', lw=2, label='Target Distribution')\n",
        "plt.title('Metropolis-Hastings Sampling')\n",
        "plt.xlabel('x')\n",
        "plt.ylabel('Density')\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "IE-2dRmwD404"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Rejection ABC Algorithm"
      ],
      "metadata": {
        "id": "SNgQIhnFEC3V"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# True parameters of the distribution (unknown to the algorithm)\n",
        "true_mu = 2.0\n",
        "true_sigma = 1.0\n",
        "\n",
        "# Simulated observed data (known to the algorithm)\n",
        "observed_data = np.random.normal(true_mu, true_sigma, size=100)\n",
        "\n",
        "# Function to simulate data from a given parameter set\n",
        "def simulate_data(mu, sigma, size=100):\n",
        "    return np.random.normal(mu, sigma, size)\n",
        "\n",
        "# Function to calculate the summary statistics (Euclidean distance in this case)\n",
        "def summary_statistics(data):\n",
        "    return np.mean(data), np.std(data)\n",
        "\n",
        "# Function to compute distance between observed and simulated summary statistics\n",
        "def compute_distance(obs_stat, sim_stat):\n",
        "    obs_mean, obs_std = obs_stat\n",
        "    sim_mean, sim_std = sim_stat\n",
        "    return np.sqrt((obs_mean - sim_mean)**2 + (obs_std - sim_std)**2)\n",
        "\n",
        "# Rejection ABC algorithm\n",
        "def rejection_abc(observed_data, epsilon, num_samples):\n",
        "    samples = []\n",
        "    while len(samples) < num_samples:\n",
        "        # Generate parameters from prior distributions (uniform here for simplicity)\n",
        "        mu = np.random.uniform(0, 5)\n",
        "        sigma = np.random.uniform(0, 3)\n",
        "\n",
        "        # Simulate data from the current parameter set\n",
        "        simulated_data = simulate_data(mu, sigma)\n",
        "\n",
        "        # Calculate summary statistics of the simulated data\n",
        "        sim_stats = summary_statistics(simulated_data)\n",
        "\n",
        "        # Calculate distance between observed and simulated summary statistics\n",
        "        distance = compute_distance(summary_statistics(observed_data), sim_stats)\n",
        "\n",
        "        # Accept the parameter set if the distance is less than epsilon\n",
        "        if distance < epsilon:\n",
        "            samples.append((mu, sigma))\n",
        "\n",
        "    return np.array(samples)\n",
        "\n",
        "# Parameters\n",
        "epsilon = 0.5  # Acceptance threshold\n",
        "num_samples = 1000  # Number of samples to generate\n",
        "\n",
        "# Run Rejection ABC algorithm\n",
        "samples = rejection_abc(observed_data, epsilon, num_samples)\n",
        "\n",
        "# Plot results\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.scatter(samples[:, 0], samples[:, 1], alpha=0.5, label='Samples')\n",
        "plt.axvline(true_mu, color='r', linestyle='--', label='True $\\mu$')\n",
        "plt.axhline(true_sigma, color='b', linestyle='--', label='True $\\sigma$')\n",
        "plt.xlabel('$\\mu$')\n",
        "plt.ylabel('$\\sigma$')\n",
        "plt.title('Rejection ABC Sampling')\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "1RnwXo-oEDSO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Probabilistic Inference in Bayesian Networks"
      ],
      "metadata": {
        "id": "2hLEA_9VEMmZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pgmpy.models import BayesianNetwork\n",
        "from pgmpy.factors.discrete import TabularCPD\n",
        "from pgmpy.inference import VariableElimination\n",
        "\n",
        "# Define the Bayesian Network structure\n",
        "bayesian_network = BayesianNetwork([('A', 'C'), ('B', 'C'), ('C', 'D')])\n",
        "\n",
        "# Define Conditional Probability Distributions (CPDs)\n",
        "cpd_a = TabularCPD(variable='A', variable_card=2, values=[[0.7], [0.3]])\n",
        "cpd_b = TabularCPD(variable='B', variable_card=2, values=[[0.8], [0.2]])\n",
        "cpd_c = TabularCPD(variable='C', variable_card=2,\n",
        "                   values=[[0.9, 0.8, 0.5, 0.4],\n",
        "                           [0.1, 0.2, 0.5, 0.6]],\n",
        "                   evidence=['A', 'B'], evidence_card=[2, 2])\n",
        "cpd_d = TabularCPD(variable='D', variable_card=2,\n",
        "                   values=[[0.3, 0.2],\n",
        "                           [0.7, 0.8]],\n",
        "                   evidence=['C'], evidence_card=[2])\n",
        "\n",
        "# Add CPDs to the Bayesian Network\n",
        "bayesian_network.add_cpds(cpd_a, cpd_b, cpd_c, cpd_d)\n",
        "\n",
        "# Check if the CPDs are valid\n",
        "print(\"CPDs valid:\", bayesian_network.check_model())\n",
        "\n",
        "# Perform variable elimination for inference\n",
        "inference = VariableElimination(bayesian_network)\n",
        "\n",
        "# Calculate the probability of variables given evidence\n",
        "query = inference.query(variables=['D'], evidence={'A': 1, 'B': 0})\n",
        "print(\"P(D | A=1, B=0):\")\n",
        "print(query)\n",
        "\n",
        "# You can query for different variables and evidence combinations as needed"
      ],
      "metadata": {
        "id": "UIEklIsIEM8s"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Metropolis-Hastings Algorithm for Bayesian Linear Regression"
      ],
      "metadata": {
        "id": "X6V6TwcwElv2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Generate synthetic data\n",
        "np.random.seed(0)\n",
        "num_samples = 100\n",
        "true_slope = 2.0\n",
        "true_intercept = 1.0\n",
        "noise_std = 1.0\n",
        "x = np.random.uniform(-5, 5, num_samples)\n",
        "y = true_slope * x + true_intercept + np.random.normal(0, noise_std, num_samples)\n",
        "\n",
        "# Bayesian Linear Regression using Metropolis-Hastings Algorithm\n",
        "def metropolis_hastings_bayesian_linear_regression(x, y, num_samples=10000, burn_in=1000):\n",
        "    # Define prior parameters (Normal distribution)\n",
        "    prior_mean = np.zeros(2)  # Mean of the prior\n",
        "    prior_cov = np.diag([1.0, 1.0])  # Covariance matrix of the prior\n",
        "\n",
        "    # Initialize parameters\n",
        "    current_beta = np.zeros(2)  # Initial guess for beta (slope and intercept)\n",
        "    beta_samples = []\n",
        "\n",
        "    # Function to calculate log likelihood\n",
        "    def log_likelihood(beta):\n",
        "        predicted = beta[0] * x + beta[1]\n",
        "        return -0.5 * np.sum((y - predicted) ** 2) / noise_std**2\n",
        "\n",
        "    # Function to calculate log prior\n",
        "    def log_prior(beta):\n",
        "        return -0.5 * (beta - prior_mean).T @ np.linalg.inv(prior_cov) @ (beta - prior_mean)\n",
        "\n",
        "    # Initial log posterior\n",
        "    current_log_posterior = log_likelihood(current_beta) + log_prior(current_beta)\n",
        "\n",
        "    # Metropolis-Hastings sampling\n",
        "    for _ in range(num_samples + burn_in):\n",
        "        # Generate proposal from a normal distribution centered at current_beta\n",
        "        proposal = current_beta + np.random.normal(0, 0.1, size=2)\n",
        "\n",
        "        # Calculate log posterior of the proposed beta\n",
        "        proposal_log_posterior = log_likelihood(proposal) + log_prior(proposal)\n",
        "\n",
        "        # Accept or reject the proposal\n",
        "        acceptance_prob = min(1, np.exp(proposal_log_posterior - current_log_posterior))\n",
        "        if np.random.uniform() < acceptance_prob:\n",
        "            current_beta = proposal\n",
        "            current_log_posterior = proposal_log_posterior\n",
        "\n",
        "        # Save samples after burn-in\n",
        "        if _ >= burn_in:\n",
        "            beta_samples.append(current_beta)\n",
        "\n",
        "    return np.array(beta_samples)\n",
        "\n",
        "# Run Metropolis-Hastings algorithm for Bayesian Linear Regression\n",
        "samples = metropolis_hastings_bayesian_linear_regression(x, y)\n",
        "\n",
        "# Plotting results\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.subplot(2, 1, 1)\n",
        "plt.plot(samples[:, 0], label='slope (beta1)')\n",
        "plt.axhline(true_slope, color='r', linestyle='--', label='True slope')\n",
        "plt.title('Samples of Slope (beta1)')\n",
        "plt.legend()\n",
        "\n",
        "plt.subplot(2, 1, 2)\n",
        "plt.plot(samples[:, 1], label='intercept (beta0)')\n",
        "plt.axhline(true_intercept, color='r', linestyle='--', label='True intercept')\n",
        "plt.title('Samples of Intercept (beta0)')\n",
        "plt.legend()\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "mJuAx61CEmQQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Bayesian Spam Filtering Algorithm"
      ],
      "metadata": {
        "id": "EKVsK2CaEpzV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "from collections import defaultdict\n",
        "import numpy as np\n",
        "\n",
        "class BayesianSpamFilter:\n",
        "    def __init__(self):\n",
        "        self.spam_word_counts = defaultdict(int)\n",
        "        self.ham_word_counts = defaultdict(int)\n",
        "        self.spam_total_count = 0\n",
        "        self.ham_total_count = 0\n",
        "        self.spam_prior = 0.5  # Prior probability of an email being spam\n",
        "        self.word_threshold = 5  # Threshold count to consider a word as a feature\n",
        "\n",
        "    def train(self, emails, labels):\n",
        "        \"\"\"\n",
        "        Train the Bayesian Spam Filter with a set of emails and their corresponding labels.\n",
        "\n",
        "        :param emails: List of email texts\n",
        "        :param labels: List of labels (1 for spam, 0 for ham)\n",
        "        \"\"\"\n",
        "        for email, label in zip(emails, labels):\n",
        "            if label == 1:\n",
        "                self.spam_total_count += 1\n",
        "                for word in self.extract_words(email):\n",
        "                    self.spam_word_counts[word] += 1\n",
        "            else:\n",
        "                self.ham_total_count += 1\n",
        "                for word in self.extract_words(email):\n",
        "                    self.ham_word_counts[word] += 1\n",
        "\n",
        "    def extract_words(self, email):\n",
        "        \"\"\"\n",
        "        Extract words from an email after cleaning and normalization.\n",
        "\n",
        "        :param email: Email text\n",
        "        :return: List of words (features)\n",
        "        \"\"\"\n",
        "        words = re.findall(r'\\b\\w+\\b', email.lower())\n",
        "        return words\n",
        "\n",
        "    def calculate_word_probabilities(self):\n",
        "        \"\"\"\n",
        "        Calculate probabilities of each word being spam or ham using Bayesian estimation.\n",
        "\n",
        "        :return: Dictionary of word probabilities\n",
        "        \"\"\"\n",
        "        word_probabilities = {}\n",
        "        for word in set(self.spam_word_counts.keys()).union(set(self.ham_word_counts.keys())):\n",
        "            spam_count = self.spam_word_counts[word]\n",
        "            ham_count = self.ham_word_counts[word]\n",
        "\n",
        "            if spam_count + ham_count >= self.word_threshold:\n",
        "                spam_probability = (spam_count / float(self.spam_total_count)) if self.spam_total_count > 0 else 0\n",
        "                ham_probability = (ham_count / float(self.ham_total_count)) if self.ham_total_count > 0 else 0\n",
        "\n",
        "                # Using Laplace (add-one) smoothing for better estimation\n",
        "                smoothing_factor = 1  # Laplace smoothing factor\n",
        "                spam_probability_smoothed = (spam_count + smoothing_factor) / \\\n",
        "                                            float(self.spam_total_count + 2 * smoothing_factor)\n",
        "                ham_probability_smoothed = (ham_count + smoothing_factor) / \\\n",
        "                                           float(self.ham_total_count + 2 * smoothing_factor)\n",
        "\n",
        "                word_probabilities[word] = {\n",
        "                    'spam_probability': spam_probability_smoothed,\n",
        "                    'ham_probability': ham_probability_smoothed\n",
        "                }\n",
        "\n",
        "        return word_probabilities\n",
        "\n",
        "    def predict(self, email):\n",
        "        \"\"\"\n",
        "        Predict whether an email is spam or ham based on its features (words).\n",
        "\n",
        "        :param email: Email text\n",
        "        :return: Predicted label (1 for spam, 0 for ham)\n",
        "        \"\"\"\n",
        "        words = self.extract_words(email)\n",
        "        word_probabilities = self.calculate_word_probabilities()\n",
        "\n",
        "        log_spam_probability = np.log(self.spam_prior)\n",
        "        log_ham_probability = np.log(1 - self.spam_prior)\n",
        "\n",
        "        for word in words:\n",
        "            if word in word_probabilities:\n",
        "                log_spam_probability += np.log(word_probabilities[word]['spam_probability'])\n",
        "                log_ham_probability += np.log(word_probabilities[word]['ham_probability'])\n",
        "\n",
        "        if log_spam_probability > log_ham_probability:\n",
        "            return 1  # Spam\n",
        "        else:\n",
        "            return 0  # Ham\n",
        "\n",
        "# Example usage:\n",
        "if __name__ == \"__main__\":\n",
        "    # Example data (emails and labels)\n",
        "    emails = [\n",
        "        \"Buy cheap Viagra now!!!\",\n",
        "        \"Hello, how are you?\",\n",
        "        \"Get free money!\",\n",
        "        \"Dear friend, please send the documents.\"\n",
        "    ]\n",
        "    labels = [1, 0, 1, 0]  # 1 for spam, 0 for ham\n",
        "\n",
        "    # Initialize and train the Bayesian Spam Filter\n",
        "    filter = BayesianSpamFilter()\n",
        "    filter.train(emails, labels)\n",
        "\n",
        "    # Test the filter with new emails\n",
        "    test_emails = [\n",
        "        \"Buy now, limited offer!\",\n",
        "        \"Hi, just checking in.\",\n",
        "        \"Send me your bank details.\",\n",
        "        \"Congratulations! You've won a prize.\"\n",
        "    ]\n",
        "\n",
        "    for email in test_emails:\n",
        "        prediction = filter.predict(email)\n",
        "        if prediction == 1:\n",
        "            print(f\"'{email}' is predicted as spam.\")\n",
        "        else:\n",
        "            print(f\"'{email}' is predicted as ham.\")"
      ],
      "metadata": {
        "id": "-BRMW8U_EqHd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Hierarchical Bayesian Model"
      ],
      "metadata": {
        "id": "ZrY7EAxBFH0U"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pymc3 as pm\n",
        "\n",
        "# Generate simulated data\n",
        "np.random.seed(42)\n",
        "num_groups = 3\n",
        "group_sizes = np.random.randint(10, 20, num_groups)\n",
        "data = []\n",
        "for i, size in enumerate(group_sizes):\n",
        "    x = np.random.randn(size)\n",
        "    y = 2 * x + np.random.randn(size)\n",
        "    data.extend(zip(x, y, [i] * size))\n",
        "\n",
        "x, y, groups = zip(*data)\n",
        "x, y, groups = np.array(x), np.array(y), np.array(groups)\n",
        "\n",
        "# Hierarchical linear regression model\n",
        "with pm.Model() as hierarchical_model:\n",
        "    # Hyperpriors\n",
        "    mu_alpha = pm.Normal('mu_alpha', mu=0, sd=10)\n",
        "    sigma_alpha = pm.HalfNormal('sigma_alpha', sd=10)\n",
        "    mu_beta = pm.Normal('mu_beta', mu=0, sd=10)\n",
        "    sigma_beta = pm.HalfNormal('sigma_beta', sd=10)\n",
        "\n",
        "    # Group-level parameters\n",
        "    alpha = pm.Normal('alpha', mu=mu_alpha, sd=sigma_alpha, shape=num_groups)\n",
        "    beta = pm.Normal('beta', mu=mu_beta, sd=sigma_beta, shape=num_groups)\n",
        "\n",
        "    # Likelihood\n",
        "    mu = alpha[groups] + beta[groups] * x\n",
        "    sigma = pm.HalfNormal('sigma', sd=1)\n",
        "    y_obs = pm.Normal('y_obs', mu=mu, sd=sigma, observed=y)\n",
        "\n",
        "# Sampling\n",
        "with hierarchical_model:\n",
        "    trace = pm.sample(2000, tune=2000)\n",
        "\n",
        "# Posterior analysis\n",
        "pm.summary(trace)\n",
        "pm.traceplot(trace)"
      ],
      "metadata": {
        "id": "X179rs1OFIX_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Implementation of Bayesian Non-parametrics"
      ],
      "metadata": {
        "id": "JYRsydIBFLoF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from scipy.stats import norm\n",
        "from sklearn.mixture import BayesianGaussianMixture\n",
        "\n",
        "# Generate simulated data\n",
        "np.random.seed(42)\n",
        "data = np.concatenate([\n",
        "    np.random.normal(-5, 1, 300),\n",
        "    np.random.normal(0, 1, 300),\n",
        "    np.random.normal(5, 1, 400)\n",
        "])\n",
        "\n",
        "# Fit Dirichlet Process Mixture Model\n",
        "dpgmm = BayesianGaussianMixture(n_components=10, covariance_type='full')\n",
        "dpgmm.fit(data.reshape(-1, 1))\n",
        "\n",
        "# Plot results\n",
        "x = np.linspace(-10, 10, 1000)\n",
        "plt.hist(data, bins=30, density=True, alpha=0.5, label='Histogram of data')\n",
        "for i in range(dpgmm.n_components):\n",
        "    y = np.exp(dpgmm.weights_[i]) * norm.pdf(x, dpgmm.means_[i, 0], np.sqrt(dpgmm.covariances_[i, 0, 0]))\n",
        "    plt.plot(x, y, label=f'Component {i}')\n",
        "plt.title('Bayesian Gaussian Mixture Model')\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "m2-mh9O9FL-s"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Implementation of Bayesian Decision Theory"
      ],
      "metadata": {
        "id": "CrmduQYxFXDt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "# Define States of the world and actions\n",
        "States = ['sunny', 'cloudy', 'rainy']\n",
        "actions = ['go_outside', 'stay_inside']\n",
        "\n",
        "# Prior probabilities over States\n",
        "prior_probs = np.array([0.4, 0.3, 0.3])\n",
        "\n",
        "# Conditional probabilities of actions given States\n",
        "action_probs = np.array([\n",
        "    [0.9, 0.1],\n",
        "    [0.5, 0.5],\n",
        "    [0.2, 0.8]\n",
        "])\n",
        "\n",
        "# Utility function\n",
        "utility = np.array([\n",
        "    [1, 0],\n",
        "    [0, 1],\n",
        "    [0, -1]\n",
        "])\n",
        "\n",
        "# Calculate expected utilities\n",
        "expected_utilities = np.dot(prior_probs, action_probs * utility)\n",
        "\n",
        "# Make decision\n",
        "decision_index = np.argmax(expected_utilities)\n",
        "decision = actions[decision_index]\n",
        "\n",
        "# Output results\n",
        "print(\"Expected Utilities:\", expected_utilities)\n",
        "print(\"Decision:\", decision)"
      ],
      "metadata": {
        "id": "4qqLpvDSFXZn"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}