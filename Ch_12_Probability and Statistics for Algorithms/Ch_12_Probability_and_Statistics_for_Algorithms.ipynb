{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Randomized QuickSort"
      ],
      "metadata": {
        "id": "dLB6xUMmH4k1"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PRkGk6pWGowC",
        "outputId": "c7b349d6-ed08-41e6-d256-101472758457"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[1, 1, 2, 3, 6, 8, 10]\n"
          ]
        }
      ],
      "source": [
        "import random\n",
        "\n",
        "def RandomizedQuickSort(A, p, r):\n",
        "    \"\"\"\n",
        "    Randomized QuickSort algorithm to sort an array.\n",
        "\n",
        "    Args:\n",
        "    A: The array to be sorted.\n",
        "    p: The starting index of the subarray to be sorted.\n",
        "    r: The ending index of the subarray to be sorted.\n",
        "    \"\"\"\n",
        "    if p < r:\n",
        "        q = RandomPartition(A, p, r)\n",
        "        RandomizedQuickSort(A, p, q - 1)\n",
        "        RandomizedQuickSort(A, q + 1, r)\n",
        "\n",
        "def RandomPartition(A, p, r):\n",
        "    \"\"\"\n",
        "    Randomly partitions the array around a pivot element.\n",
        "\n",
        "    Args:\n",
        "    A: The array to be partitioned.\n",
        "    p: The starting index of the subarray to be partitioned.\n",
        "    r: The ending index of the subarray to be partitioned.\n",
        "\n",
        "    Returns:\n",
        "    The index of the pivot element after partitioning.\n",
        "    \"\"\"\n",
        "    i = random.randint(p, r)\n",
        "    A[i], A[r] = A[r], A[i]\n",
        "    return Partition(A, p, r)\n",
        "\n",
        "def Partition(A, p, r):\n",
        "    \"\"\"\n",
        "    Partitions the array around the pivot element.\n",
        "\n",
        "    Args:\n",
        "    A: The array to be partitioned.\n",
        "    p: The starting index of the subarray to be partitioned.\n",
        "    r: The ending index of the subarray to be partitioned.\n",
        "\n",
        "    Returns:\n",
        "    The index of the pivot element after partitioning.\n",
        "    \"\"\"\n",
        "    x = A[r]\n",
        "    i = p - 1\n",
        "    for j in range(p, r):\n",
        "        if A[j] <= x:\n",
        "            i += 1\n",
        "            A[i], A[j] = A[j], A[i]\n",
        "    A[i + 1], A[r] = A[r], A[i + 1]\n",
        "    return i + 1\n",
        "\n",
        "# Example usage:\n",
        "A = [3, 6, 8, 10, 1, 2, 1]\n",
        "RandomizedQuickSort(A, 0, len(A) - 1)\n",
        "print(A)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Monte Carlo Estimation"
      ],
      "metadata": {
        "id": "6aCtpctgINQA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import random\n",
        "\n",
        "def monte_carlo_estimation(N):\n",
        "    \"\"\"\n",
        "    Monte Carlo estimation to compute the expected value of a function.\n",
        "\n",
        "    Args:\n",
        "    N: The number of samples.\n",
        "\n",
        "    Returns:\n",
        "    The estimated mean (expected value) of the function f(x).\n",
        "    \"\"\"\n",
        "    S = 0\n",
        "    for i in range(1, N + 1):\n",
        "        X_i = random.random()  # Generate random sample\n",
        "        f_X_i = f(X_i)  # Compute f(X_i)\n",
        "        S += f_X_i  # Update sum S\n",
        "    mu_hat = S / N  # Compute the estimated mean\n",
        "    return mu_hat\n",
        "\n",
        "# Define the function f(x)\n",
        "def f(x):\n",
        "    \"\"\"\n",
        "    Function to be estimated.\n",
        "\n",
        "    Args:\n",
        "    x: Input value.\n",
        "\n",
        "    Returns:\n",
        "    The value of the function f(x).\n",
        "    \"\"\"\n",
        "    # Example function, replace with your own function definition\n",
        "    return x ** 2\n",
        "\n",
        "# Example usage\n",
        "N = 1000  # Number of samples\n",
        "estimated_mean = monte_carlo_estimation(N)\n",
        "print(\"Estimated mean:\", estimated_mean)"
      ],
      "metadata": {
        "id": "HAQ6N-UdINwi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Las Vegas Algorithm"
      ],
      "metadata": {
        "id": "zDqrxb24IZpd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import random\n",
        "\n",
        "def LasVegasAlgorithm():\n",
        "    \"\"\"\n",
        "    Las Vegas algorithm that repeatedly executes a randomized algorithm until a termination condition is met.\n",
        "\n",
        "    Returns:\n",
        "    The result of the algorithm once the termination condition is met.\n",
        "    \"\"\"\n",
        "    result = None\n",
        "    while not termination_condition_met():\n",
        "        # Make random choices\n",
        "        # Execute algorithm\n",
        "        # Update result if necessary\n",
        "        result = execute_algorithm()\n",
        "    return result\n",
        "\n",
        "def termination_condition_met():\n",
        "    \"\"\"\n",
        "    Define your termination condition here.\n",
        "\n",
        "    Returns:\n",
        "    A boolean indicating whether the termination condition is met.\n",
        "    \"\"\"\n",
        "    # Example: Replace with actual termination condition\n",
        "    return False\n",
        "\n",
        "def execute_algorithm():\n",
        "    \"\"\"\n",
        "    Implement your algorithm here.\n",
        "\n",
        "    Returns:\n",
        "    The result of your algorithm.\n",
        "    \"\"\"\n",
        "    # Example: Replace with the actual algorithm\n",
        "    return random.choice([True, False])  # Placeholder example\n",
        "\n",
        "# Example usage:\n",
        "result = LasVegasAlgorithm()\n",
        "print(\"Result:\", result)"
      ],
      "metadata": {
        "id": "dhKRwGY6IaCT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Expectation Maximization Algorithm"
      ],
      "metadata": {
        "id": "6vOorantIl26"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def expectation_maximization():\n",
        "    \"\"\"\n",
        "    Expectation-Maximization (EM) algorithm to estimate model parameters.\n",
        "\n",
        "    This function repeatedly performs the expectation and maximization steps\n",
        "    until convergence criteria are met.\n",
        "    \"\"\"\n",
        "    # Initialize model parameters\n",
        "    initialize_model_parameters()\n",
        "\n",
        "    converged = False\n",
        "    while not converged:\n",
        "        # Expectation Step: Compute expected values of hidden variables\n",
        "        compute_expected_values()\n",
        "\n",
        "        # Maximization Step: Update model parameters to maximize likelihood\n",
        "        update_model_parameters()\n",
        "\n",
        "        # Check for convergence (e.g., change in likelihood)\n",
        "        if convergence_criteria_met():\n",
        "            converged = True\n",
        "\n",
        "def initialize_model_parameters():\n",
        "    \"\"\"\n",
        "    Initialize model parameters.\n",
        "\n",
        "    This function performs the initialization of model parameters.\n",
        "    \"\"\"\n",
        "    # Example: Initialize parameters randomly or based on some heuristic\n",
        "    global parameters\n",
        "    parameters = {\n",
        "        'mean': random.random(),\n",
        "        'variance': random.random(),\n",
        "        'mixing_coeff': random.random()\n",
        "    }\n",
        "\n",
        "def compute_expected_values():\n",
        "    \"\"\"\n",
        "    Compute expected values of hidden variables.\n",
        "\n",
        "    This function performs the expectation step to compute the expected values of hidden variables.\n",
        "    \"\"\"\n",
        "    # Example: Compute expected values based on current parameters\n",
        "    global expected_values\n",
        "    expected_values = {\n",
        "        'latent_var_1': parameters['mean'] * 0.5,\n",
        "        'latent_var_2': parameters['variance'] * 0.5\n",
        "    }\n",
        "\n",
        "def update_model_parameters():\n",
        "    \"\"\"\n",
        "    Update model parameters.\n",
        "\n",
        "    This function performs the maximization step to update model parameters.\n",
        "    \"\"\"\n",
        "    # Example: Update parameters to maximize likelihood\n",
        "    global parameters\n",
        "    parameters['mean'] += 0.1\n",
        "    parameters['variance'] += 0.1\n",
        "    parameters['mixing_coeff'] = 1 - parameters['mixing_coeff']\n",
        "\n",
        "def convergence_criteria_met():\n",
        "    \"\"\"\n",
        "    Check if convergence criteria are met.\n",
        "\n",
        "    This function checks if the convergence criteria are met (e.g., change in likelihood).\n",
        "\n",
        "    Returns:\n",
        "    A boolean indicating whether the convergence criteria are met.\n",
        "    \"\"\"\n",
        "    # Example: Check if the parameters have converged\n",
        "    global parameters\n",
        "    return abs(parameters['mean'] - parameters['variance']) < 0.01\n",
        "\n",
        "# Example usage\n",
        "expectation_maximization()"
      ],
      "metadata": {
        "id": "s_BxnkLrImYv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Sample Mean Estimation"
      ],
      "metadata": {
        "id": "PXl_KZNrIz6H"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def sample_mean_estimation(sample_data):\n",
        "    \"\"\"\n",
        "    Estimate the sample mean from the given sample data.\n",
        "\n",
        "    Args:\n",
        "    sample_data: A list of numerical values representing the sample data.\n",
        "\n",
        "    Returns:\n",
        "    The sample mean of the given data.\n",
        "    \"\"\"\n",
        "    n = len(sample_data)  # Number of samples\n",
        "    S = sum(sample_data)  # Initialize sum S\n",
        "    sample_mean = S / n  # Compute sample mean \\bar{X}\n",
        "    return sample_mean\n",
        "\n",
        "# Example usage:\n",
        "sample_data = [10, 20, 30, 40, 50]\n",
        "sample_mean = sample_mean_estimation(sample_data)\n",
        "print(\"Sample Mean:\", sample_mean)"
      ],
      "metadata": {
        "id": "RJJh3n_PI0V0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Confidence Interval Estimation"
      ],
      "metadata": {
        "id": "74gMhRoTI_nk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from scipy.stats import norm\n",
        "\n",
        "def confidence_interval_estimation(data, alpha=0.05):\n",
        "    \"\"\"\n",
        "    Estimate the confidence interval for the mean of the given data.\n",
        "\n",
        "    Args:\n",
        "    data: A list or array of numerical values representing the sample data.\n",
        "    alpha: The significance level (default is 0.05 for a 95% confidence interval).\n",
        "\n",
        "    Returns:\n",
        "    The lower and upper bounds of the confidence interval.\n",
        "    \"\"\"\n",
        "    n = len(data)  # Number of samples\n",
        "    sample_mean = np.mean(data)  # Sample mean\n",
        "    sample_std = np.std(data, ddof=1)  # Sample standard deviation (ddof=1 for sample std deviation)\n",
        "    z_alpha_half = norm.ppf(1 - alpha / 2)  # z-value for the given confidence level\n",
        "\n",
        "    margin_of_error = z_alpha_half * sample_std / np.sqrt(n)  # Margin of error\n",
        "    lower_bound = sample_mean - margin_of_error  # Lower bound of confidence interval\n",
        "    upper_bound = sample_mean + margin_of_error  # Upper bound of confidence interval\n",
        "\n",
        "    return lower_bound, upper_bound\n",
        "\n",
        "# Example usage:\n",
        "sample_data = [10, 15, 20, 25, 30]\n",
        "confidence_level = 0.95\n",
        "lower_bound, upper_bound = confidence_interval_estimation(sample_data, alpha=1 - confidence_level)\n",
        "print(\"Confidence Interval:\", (lower_bound, upper_bound))"
      ],
      "metadata": {
        "id": "N1npdMvwI__a"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Random Walk on a Graph"
      ],
      "metadata": {
        "id": "HPZDKtQQJWIO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import random\n",
        "\n",
        "def random_walk(graph, starting_vertex, T):\n",
        "    \"\"\"\n",
        "    Perform a random walk on a graph.\n",
        "\n",
        "    Args:\n",
        "    graph: A dictionary representing the graph where keys are vertex names and values are lists of adjacent vertices.\n",
        "    starting_vertex: The vertex where the random walk starts.\n",
        "    T: The number of steps in the random walk.\n",
        "\n",
        "    Returns:\n",
        "    The vertex reached after T steps or the last vertex if the walk terminates early due to no neighbors.\n",
        "    \"\"\"\n",
        "    current_vertex = starting_vertex\n",
        "    for t in range(T):\n",
        "        neighbors = graph.get(current_vertex, [])  # Get the neighbors of the current vertex\n",
        "        if neighbors:\n",
        "            next_vertex = random.choice(neighbors)  # Choose a random neighbor\n",
        "            current_vertex = next_vertex  # Move to the next vertex\n",
        "        else:\n",
        "            break  # If there are no neighbors, terminate the walk\n",
        "    return current_vertex\n",
        "\n",
        "# Example usage:\n",
        "graph = {\n",
        "    'A': ['B', 'C'],\n",
        "    'B': ['A', 'C', 'D'],\n",
        "    'C': ['A', 'B', 'D'],\n",
        "    'D': ['B', 'C']\n",
        "}\n",
        "starting_vertex = 'A'\n",
        "T = 10\n",
        "final_vertex = random_walk(graph, starting_vertex, T)\n",
        "print(\"Final Vertex after Random Walk:\", final_vertex)"
      ],
      "metadata": {
        "id": "X0jeO5A_JW1e"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Bayesian Linear Regression"
      ],
      "metadata": {
        "id": "ETF_6D9_J1jx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import scipy.stats as stats\n",
        "\n",
        "def bayesian_linear_regression(X, Y, prior_mean, prior_variance, noise_variance, num_samples=1000):\n",
        "    \"\"\"\n",
        "    Perform Bayesian linear regression to estimate the parameters and make predictions.\n",
        "\n",
        "    Args:\n",
        "    X: The feature matrix.\n",
        "    Y: The target values.\n",
        "    prior_mean: The prior mean for the parameters theta.\n",
        "    prior_variance: The prior variance for the parameters theta.\n",
        "    noise_variance: The variance of the noise in the data.\n",
        "    num_samples: The number of samples to draw from the posterior distribution (default is 1000).\n",
        "\n",
        "    Returns:\n",
        "    estimated_parameters: The estimated parameters theta with the highest posterior probability.\n",
        "    predicted_Y_new: The predicted target values for the given feature matrix X.\n",
        "    \"\"\"\n",
        "    # Prior distribution for parameters theta\n",
        "    prior_distribution = stats.norm(loc=prior_mean, scale=np.sqrt(prior_variance))\n",
        "\n",
        "    # Likelihood function P(Y | X, theta)\n",
        "    def likelihood(theta):\n",
        "        predicted_Y = np.dot(X, theta)\n",
        "        likelihoods = stats.norm.logpdf(Y, loc=predicted_Y, scale=np.sqrt(noise_variance))\n",
        "        return np.sum(likelihoods)\n",
        "\n",
        "    # Calculate posterior distribution P(theta | X, Y) using Bayes' theorem\n",
        "    def posterior(theta):\n",
        "        return prior_distribution.logpdf(theta).sum() + likelihood(theta)\n",
        "\n",
        "    # Sample from the prior distribution\n",
        "    samples = np.random.normal(prior_mean, np.sqrt(prior_variance), size=(num_samples, len(prior_mean)))\n",
        "    posterior_samples = [(sample, posterior(sample)) for sample in samples]\n",
        "\n",
        "    # Sort samples by their posterior probability in descending order\n",
        "    posterior_samples.sort(key=lambda x: x[1], reverse=True)\n",
        "\n",
        "    # Use estimated parameters to make predictions for new data points\n",
        "    estimated_parameters = posterior_samples[0][0]  # Take the sample with highest posterior probability\n",
        "    predicted_Y_new = np.dot(X, estimated_parameters)\n",
        "\n",
        "    return estimated_parameters, predicted_Y_new\n",
        "\n",
        "# Example usage:\n",
        "X = np.array([[1, 2], [3, 4], [5, 6]])  # Example feature matrix\n",
        "Y = np.array([3, 4, 5])  # Example target values\n",
        "prior_mean = np.array([0, 0])  # Example prior mean for parameters theta\n",
        "prior_variance = 1.0  # Example prior variance for parameters theta\n",
        "noise_variance = 0.1  # Example noise variance\n",
        "estimated_parameters, predicted_Y_new = bayesian_linear_regression(X, Y, prior_mean, prior_variance, noise_variance)\n",
        "print(\"Estimated Parameters:\", estimated_parameters)\n",
        "print(\"Predicted Y for New Data Points:\", predicted_Y_new)"
      ],
      "metadata": {
        "id": "eVGL8tQvJ2B8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Belief Propagation in Bayesian Networks"
      ],
      "metadata": {
        "id": "RVIVgsROKGzM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "def belief_propagation(bayesian_network, max_iterations=100, tolerance=1e-6):\n",
        "    \"\"\"\n",
        "    Perform belief propagation in a Bayesian network.\n",
        "\n",
        "    Args:\n",
        "    bayesian_network: A list of nodes representing the Bayesian network. Each node has a list of neighbors.\n",
        "    max_iterations: The maximum number of iterations to run the algorithm (default is 100).\n",
        "    tolerance: The convergence tolerance (default is 1e-6).\n",
        "\n",
        "    Returns:\n",
        "    beliefs: The final beliefs for each node.\n",
        "    messages: The messages passed between nodes during belief propagation.\n",
        "    \"\"\"\n",
        "    # Initialize messages at each node\n",
        "    messages = initialize_messages(bayesian_network)\n",
        "    beliefs = np.zeros_like(messages)\n",
        "\n",
        "    # Convergence flag\n",
        "    converged = False\n",
        "    iteration = 0\n",
        "\n",
        "    while not converged and iteration < max_iterations:\n",
        "        converged = True\n",
        "\n",
        "        # Iterate over nodes in topological order\n",
        "        for i, node in enumerate(bayesian_network):\n",
        "            incoming_messages = []\n",
        "\n",
        "            # Collect incoming messages from neighbors\n",
        "            for neighbor in node.neighbors:\n",
        "                incoming_messages.append(messages[neighbor][i])\n",
        "\n",
        "            # Send message to node i from each neighbor\n",
        "            for neighbor in node.neighbors:\n",
        "                message_to_i = compute_message(beliefs, incoming_messages, node, neighbor)\n",
        "                if np.linalg.norm(messages[neighbor][i] - message_to_i) > tolerance:\n",
        "                    converged = False\n",
        "                messages[neighbor][i] = message_to_i\n",
        "\n",
        "            # Update beliefs at node i based on incoming messages\n",
        "            beliefs[i] = update_beliefs(node, incoming_messages)\n",
        "\n",
        "        iteration += 1\n",
        "\n",
        "    return beliefs, messages\n",
        "\n",
        "def initialize_messages(bayesian_network):\n",
        "    \"\"\"\n",
        "    Initialize messages for the Bayesian network.\n",
        "\n",
        "    Args:\n",
        "    bayesian_network: A list of nodes representing the Bayesian network.\n",
        "\n",
        "    Returns:\n",
        "    A numpy array of initial messages.\n",
        "    \"\"\"\n",
        "    num_nodes = len(bayesian_network)\n",
        "    return np.zeros((num_nodes, num_nodes))\n",
        "\n",
        "def compute_message(beliefs, incoming_messages, node_i, node_j):\n",
        "    \"\"\"\n",
        "    Compute the message from node_j to node_i.\n",
        "\n",
        "    Args:\n",
        "    beliefs: The current beliefs for each node.\n",
        "    incoming_messages: The incoming messages to node_i.\n",
        "    node_i: The target node.\n",
        "    node_j: The source node.\n",
        "\n",
        "    Returns:\n",
        "    The message from node_j to node_i.\n",
        "    \"\"\"\n",
        "    # Example computation of message from node_j to node_i\n",
        "    return np.ones_like(beliefs[node_i]) * 0.5\n",
        "\n",
        "def update_beliefs(node, incoming_messages):\n",
        "    \"\"\"\n",
        "    Update the beliefs at a node based on incoming messages.\n",
        "\n",
        "    Args:\n",
        "    node: The current node.\n",
        "    incoming_messages: The incoming messages to the node.\n",
        "\n",
        "    Returns:\n",
        "    The updated beliefs for the node.\n",
        "    \"\"\"\n",
        "    # Example update of beliefs at node_i based on incoming messages\n",
        "    return np.ones_like(incoming_messages[0])\n",
        "\n",
        "# Example usage:\n",
        "class Node:\n",
        "    def __init__(self, neighbors):\n",
        "        self.neighbors = neighbors\n",
        "\n",
        "# Example Bayesian Network represented as a list of nodes\n",
        "bayesian_network = [Node([1]), Node([0, 2]), Node([1])]\n",
        "\n",
        "beliefs, messages = belief_propagation(bayesian_network)\n",
        "print(\"Beliefs:\", beliefs)\n",
        "print(\"Messages:\", messages)"
      ],
      "metadata": {
        "id": "XgAA7By_KHOr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# PageRank Algorithm"
      ],
      "metadata": {
        "id": "lbL0NjfVK4WL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "def pagerank(graph, damping_factor=0.85, max_iterations=100, tolerance=1e-6):\n",
        "    \"\"\"\n",
        "    Compute the PageRank of each node in the graph.\n",
        "\n",
        "    Args:\n",
        "    graph: A dictionary representing the graph where keys are nodes and values are lists of adjacent nodes.\n",
        "    damping_factor: The damping factor (default is 0.85).\n",
        "    max_iterations: The maximum number of iterations to run the algorithm (default is 100).\n",
        "    tolerance: The convergence tolerance (default is 1e-6).\n",
        "\n",
        "    Returns:\n",
        "    pagerank_values: A dictionary with nodes as keys and their corresponding PageRank values as values.\n",
        "    \"\"\"\n",
        "    num_nodes = len(graph)\n",
        "    pagerank_values = {node: 1 / num_nodes for node in graph}  # Initialize PageRank values\n",
        "    new_pagerank_values = pagerank_values.copy()\n",
        "\n",
        "    for iteration in range(max_iterations):\n",
        "        for node in graph:\n",
        "            rank_sum = 0\n",
        "            for neighbor in graph:\n",
        "                if node in graph[neighbor]:\n",
        "                    rank_sum += pagerank_values[neighbor] / len(graph[neighbor])\n",
        "            new_pagerank_values[node] = (1 - damping_factor) / num_nodes + damping_factor * rank_sum\n",
        "\n",
        "        # Check for convergence\n",
        "        if all(abs(new_pagerank_values[node] - pagerank_values[node]) < tolerance for node in graph):\n",
        "            break\n",
        "\n",
        "        pagerank_values = new_pagerank_values.copy()\n",
        "\n",
        "    return pagerank_values\n",
        "\n",
        "# Example usage:\n",
        "graph = {\n",
        "    'A': ['B', 'C'],\n",
        "    'B': ['C', 'D'],\n",
        "    'C': ['A'],\n",
        "    'D': ['C']\n",
        "}\n",
        "\n",
        "pagerank_values = pagerank(graph)\n",
        "print(\"PageRank Values:\", pagerank_values)"
      ],
      "metadata": {
        "id": "YHUtC5fPK5t5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Count-Min Sketch Algorithm"
      ],
      "metadata": {
        "id": "DEslOM8GK-1y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import hashlib\n",
        "\n",
        "class CountMinSketch:\n",
        "    def __init__(self, w, k):\n",
        "        \"\"\"\n",
        "        Initialize the Count-Min Sketch.\n",
        "\n",
        "        Args:\n",
        "        w: The width of the counters array.\n",
        "        k: The number of hash functions.\n",
        "        \"\"\"\n",
        "        self.w = w  # Width of the array\n",
        "        self.k = k  # Number of hash functions\n",
        "        self.C = [[0] * w for _ in range(k)]  # Initialize the counters array\n",
        "\n",
        "    def hash_functions(self, x):\n",
        "        \"\"\"\n",
        "        Generate k different hash values for the input x.\n",
        "\n",
        "        Args:\n",
        "        x: The input element to be hashed.\n",
        "\n",
        "        Returns:\n",
        "        A list of k hash values.\n",
        "        \"\"\"\n",
        "        hashes = []\n",
        "        for i in range(self.k):\n",
        "            # Use different hash functions by appending the index to the string\n",
        "            hashes.append(int(hashlib.md5((str(x) + str(i)).encode()).hexdigest(), 16) % self.w)\n",
        "        return hashes\n",
        "\n",
        "    def update(self, x):\n",
        "        \"\"\"\n",
        "        Update the Count-Min Sketch with the input x.\n",
        "\n",
        "        Args:\n",
        "        x: The input element to be added to the sketch.\n",
        "        \"\"\"\n",
        "        for i, hash_val in enumerate(self.hash_functions(x)):\n",
        "            self.C[i][hash_val] += 1\n",
        "\n",
        "    def query(self, x):\n",
        "        \"\"\"\n",
        "        Query the frequency of the input x in the Count-Min Sketch.\n",
        "\n",
        "        Args:\n",
        "        x: The input element to query.\n",
        "\n",
        "        Returns:\n",
        "        The estimated frequency of x.\n",
        "        \"\"\"\n",
        "        min_count = float('inf')\n",
        "        for i, hash_val in enumerate(self.hash_functions(x)):\n",
        "            min_count = min(min_count, self.C[i][hash_val])\n",
        "        return min_count\n",
        "\n",
        "# Example usage\n",
        "data_stream = [1, 2, 3, 4, 1, 2, 1, 2, 3, 4, 4, 4]\n",
        "w = 10  # Width of the array\n",
        "k = 5   # Number of hash functions\n",
        "cms = CountMinSketch(w, k)\n",
        "\n",
        "# Update the Count-Min Sketch with the data stream\n",
        "for element in data_stream:\n",
        "    cms.update(element)\n",
        "\n",
        "# Query the frequency of elements\n",
        "query_elements = [1, 2, 3, 4, 5]\n",
        "for element in query_elements:\n",
        "    print(\"Frequency of\", element, \":\", cms.query(element))\n"
      ],
      "metadata": {
        "id": "u9yPJ5V9K_M-"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}